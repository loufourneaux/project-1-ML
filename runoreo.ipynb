{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74564550-2330-44e7-8a86-a088dcbf0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beeaf269-ba70-4922-9de9-80766dcb59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = \"data/\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "    # Load and prepare data for training:\n",
    "  #  y, x, ids= load_csv_data(DATA_PATH + \"train.csv\", sub_sample=False)\n",
    "    \n",
    "    #call les fonctions de filtre de adrien\n",
    "    #data = filter_data(y, x,ids )*\n",
    "    #outliers\n",
    "    #ect, tout call ici\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d218da50-1c35-4220-a717-56d6a0ee808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Pour tester les fonctions on va utiliser la data des séries pour l'instant\"\"\"\n",
    "\n",
    "from helperstemporaire import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6996b8-8955-49ec-acd9-24c4798ece99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#checker la gueule de la data: faire un avant après avec des histogrammes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"#checker la gueule de la data: faire un avant après avec des histogrammes\"\n",
    "\n",
    " #hist_before = plt.hist(x[:,i])\n",
    " #plt.show()\n",
    "    #hist_before = plt.hist(x[:,i])\n",
    "    #plt.show()\n",
    "    #hist_after = plt.hist(x[:,i])\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c66fa8-c3c6-4cc6-9f4d-33fb3bab250b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Toutes les fonctions dans le fichier implementations (problème pour call les fonctions dans implémentations\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm using MSE loss.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        w: model parameters as numpy arrays of shape (D, )\n",
    "        loss: loss mse value (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights and loss\n",
    "    w = initial_w\n",
    "    loss = compute_loss(y, tx, w, \"mse\")\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # compute gradient\n",
    "        grad = compute_gradient(y, tx, w, \"mse\")\n",
    "\n",
    "        # update w by gradient descent\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        # compute loss\n",
    "        loss = compute_loss(y, tx, w, \"mse\")\n",
    "\n",
    "        # Display current loss\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}\".format(bi=i, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD) using MSE loss.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        w: model parameters as numpy arrays of shape (D, )\n",
    "        loss: loss mse value (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights and loss\n",
    "    w = initial_w\n",
    "    loss = compute_loss(y, tx, w, \"mse\")\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "\n",
    "            # compute gradient\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w, \"mse\")\n",
    "\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w, \"mse\")\n",
    "\n",
    "        # Display current loss\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss)\n",
    "        )\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "    returns mse, and optimal weights.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "\n",
    "    Returns:\n",
    "        w: model parameters as numpy arrays of shape (D, )\n",
    "        loss: loss mse value (scalar)\n",
    "\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
    "    loss = compute_loss(y, tx, w, \"mse\")\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        loss: loss mse value (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    l = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    w = np.linalg.solve(tx.T @ tx + l, tx.T @ y)\n",
    "    loss = compute_loss(y, tx, w, \"mse\")\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Logistic regression using GD\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        w: model parameters as numpy arrays of shape (D, )\n",
    "        loss: log-loss value (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights and loss\n",
    "    w = initial_w\n",
    "    loss = compute_loss(y, tx, w, \"log\")\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # compute gradient\n",
    "        grad = compute_gradient(y, tx, w, \"log\")\n",
    "\n",
    "        # update w through the stochastic gradient update\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        # calculate loss\n",
    "        loss = compute_loss(y, tx, w, \"log\")\n",
    "\n",
    "        # Display current loss\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}\".format(\n",
    "                bi=i, ti=max_iters - 1, l=loss\n",
    "            )\n",
    "        )\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Regularized logistic regression using GD\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        lambda_: scalar.\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        w: model parameters as numpy arrays of shape (D, )\n",
    "        loss: log-loss value (scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights and loss\n",
    "    w = initial_w\n",
    "    loss = compute_loss(y, tx, w, \"log\")\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # compute gradient\n",
    "        grad = compute_gradient(y, tx, w, \"log\", lambda_=lambda_)\n",
    "\n",
    "        # update w through the stochastic gradient update\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        # calculate loss\n",
    "        loss = compute_loss(y, tx, w, \"log\")\n",
    "\n",
    "        # Display current loss and weights\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}\".format(bi=i, ti=max_iters - 1, l=loss))\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def compute_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return np.mean(e**2) / 2\n",
    "\n",
    "\n",
    "def compute_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def calculate_logloss(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"Calculate the logloss\"\"\"\n",
    "    return -np.mean(\n",
    "        y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps)\n",
    "    )\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w, loss_type):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2,). The vector of model parameters.\n",
    "        loss_type: string in [\"mae\", \"mse\", \"log\"] specifying the type of loss to compute\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "\n",
    "    e = y -tx.dot(w)\n",
    "\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        return calculate_mse(e)\n",
    "\n",
    "    elif loss_type == \"mae\":\n",
    "        return calculate_mae(e)\n",
    "\n",
    "    elif loss_type == \"log\":\n",
    "        y_pred = sigmoid(tx @ w)\n",
    "        return calculate_logloss(y, y_pred)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid value for argument 'loss_type' when calling compute_loss, 'type' must be in ['mse', 'mae', 'log'].\"\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w, loss_type, lambda_=0):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        w: numpy array of shape=(D, ). The vector of model parameters.\n",
    "        loss_type: string in [\"mse\", \"log\"] specifying the type of loss\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (D, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        e = y - tx @ w\n",
    "        grad = -(tx.T @ e) / y.shape[0]\n",
    "\n",
    "    elif loss_type == \"log\":\n",
    "        e = sigmoid(tx @ w) - y\n",
    "        grad = (tx.T @ e) / y.shape[0]\n",
    "        grad = grad + 2 * lambda_ * w\n",
    "    return grad\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size=1, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb230a-9e55-4630-845b-910e42a9c1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"fonctions utiles pour faire la cross validation\"\"\"\n",
    "from utils import build_poly\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "\n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "\n",
    "    >>> build_poly(np.array([0.0, 1.5]), 2)\n",
    "    array([[1.  , 0.  , 0.  ],\n",
    "           [1.  , 1.5 , 2.25]])\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number of samples is not round\n",
    "    you can use np.floor. Also check the documentation for np.random.permutation,\n",
    "    it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "\n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]), array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    ### SOLUTION\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[:index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SOLUTION\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    \n",
    "    # form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    # ridge regression\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_loss(y_tr, tx_tr, w, \"mse\"))\n",
    "    loss_te = np.sqrt(2 * compute_loss(y_te, tx_te, w, \"mse\"))\n",
    "\n",
    "    return loss_tr, loss_te\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef51c96-1462-40ac-a0ed-d569ad09f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"cross validation avec ridge regression\" #fonctions nécessaires\n",
    "  \n",
    "#cette cross validation permet d'ajuster les degré du polynôme et lambda en même temps\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Attention cette cross validation  est différente de la best_degree_selection des séries, elle revoie aussi tout les rmse\"\"\"\n",
    "def best_degree_selection(degrees, k_fold, lambdas, seed=1):\n",
    "    \"\"\"Cross-validation over regularization parameter lambda and degree.\n",
    "    \n",
    "    Args:\n",
    "        degrees: shape=(d,), where d is the number of degrees to test\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape=(p,), where p is the number of values of lambda to test\n",
    "        seed: integer, random seed\n",
    "    \n",
    "    Returns:\n",
    "        best_degree : integer, value of the best degree\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_degree, best_lambda)\n",
    "    \"\"\"\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    best_lambdas = np.zeros((len(degrees), len(lambdas)))\n",
    "    best_rmses = np.zeros((len(degrees), len(lambdas)))\n",
    "\n",
    "    for i, degree in enumerate(degrees):\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            rmse_te = []\n",
    "            for k in range(k_fold):\n",
    "                _, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                rmse_te.append(loss_te)\n",
    "            best_lambdas[i, j] = lambda_\n",
    "            best_rmses[i, j] = np.mean(rmse_te)\n",
    "\n",
    "    ind_best_degree, ind_best_lambda = np.unravel_index(np.argmin(best_rmses), best_rmses.shape)\n",
    "    best_degree = degrees[ind_best_degree]\n",
    "    best_lambda = lambdas[ind_best_lambda]\n",
    "    best_rmse = best_rmses[ind_best_degree, ind_best_lambda]\n",
    "\n",
    "    return best_degree, best_lambda, best_rmse, best_lambdas, best_rmses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98178dc2-aea9-41f0-8b00-38037dc31188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m lambdas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m best_degree, best_lambda, best_rmse, best_lambdas, best_rmses \u001b[38;5;241m=\u001b[39m \u001b[43mbest_degree_selection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create a meshgrid of degrees and lambdas\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(lambdas, degrees)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mbest_degree_selection\u001b[0;34m(degrees, k_fold, lambdas, seed)\u001b[0m\n\u001b[1;32m     29\u001b[0m rmse_te \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[0;32m---> 31\u001b[0m     _, loss_te \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     rmse_te\u001b[38;5;241m.\u001b[39mappend(loss_te)\n\u001b[1;32m     33\u001b[0m best_lambdas[i, j] \u001b[38;5;241m=\u001b[39m lambda_\n",
      "Cell \u001b[0;32mIn[6], line 119\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[1;32m    117\u001b[0m tx_te \u001b[38;5;241m=\u001b[39m build_poly(x_te, degree)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# ridge regression\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mridge_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# calculate the loss for train and test data\u001b[39;00m\n\u001b[1;32m    121\u001b[0m loss_tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m compute_loss(y_tr, tx_tr, w, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 114\u001b[0m, in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m    112\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m tx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m lambda_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(tx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    113\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(tx\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m tx \u001b[38;5;241m+\u001b[39m l, tx\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y)\n\u001b[0;32m--> 114\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w, loss\n",
      "Cell \u001b[0;32mIn[5], line 231\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w, loss_type)\u001b[0m\n\u001b[1;32m    227\u001b[0m e \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39mtx\u001b[38;5;241m.\u001b[39mdot(w)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculate_mse\u001b[49m(e)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m loss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_mae(e)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_mse' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Visualisation de la cross validation pour lambda_ et les degrés\"\"\"\n",
    "\n",
    "# Sample degrees and lambdas for visualization\n",
    "degrees = np.arange(2, 11)\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "\n",
    "best_degree, best_lambda, best_rmse, best_lambdas, best_rmses = best_degree_selection(\n",
    "    degrees, k_fold=4, lambdas=lambdas, seed=seed\n",
    ")\n",
    "\n",
    "# Create a meshgrid of degrees and lambdas\n",
    "X, Y = np.meshgrid(lambdas, degrees)\n",
    "\n",
    "# Visualize RMSE values as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "c = plt.contourf(X, Y, best_rmses, levels=30, cmap='viridis')\n",
    "plt.colorbar(c, label='RMSE')\n",
    "\n",
    "# Mark the best degree and lambda with a red dot\n",
    "plt.plot(best_lambda, best_degree, 'ro', markersize=8, label=f'Best RMSE ({best_degree}, {best_lambda:.4f})')\n",
    "\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Degree')\n",
    "plt.xscale('log')\n",
    "plt.title('Cross-Validation Results for Different Degrees and Lambdas')\n",
    "\n",
    "# Enhance graph aesthetics\n",
    "plt.xticks(lambdas, ['%.2E' % Decimal(l) for l in lambdas], rotation=45)\n",
    "plt.yticks(degrees)\n",
    "plt.gca().invert_yaxis()  # Invert Y-axis for better readability\n",
    "\n",
    "plt.savefig(\"cross_validation.png\")  # Save the plot as an image\n",
    "\n",
    "print(\n",
    "    \"The best RMSE of %.3f is obtained for a degree of %.f and a lambda of %.5f.\"\n",
    "    % (best_rmse, best_degree, best_lambda)\n",
    ")\n",
    "\n",
    "plt.legend()  # Display the legend only once\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f07dce-e72b-4961-9399-2bed9ac0fadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"cross validation least squares same as exo 3, allows to tune degrees and the split ratio\"\"\"\n",
    "\n",
    "import build_poly\n",
    "from implementations import compute_loss\n",
    "from implementations import least_squares\n",
    "from implementations import calculate_mse #nom à changer \n",
    "from implementations import compute_loss #paramètre dedans pour faire mar ou mse\n",
    "\n",
    "#spécifique à least squares\n",
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\n",
    "\n",
    "    Returns:\n",
    "      x_tr: numpy array\n",
    "      x_te: numpy array\n",
    "      y_tr: numpy array\n",
    "      y_te: numpy array\n",
    "      weights: weights from the least squares optimization\"\"\"\n",
    "    ### SOLUTION\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    w, mse_tr = least_squares(y_tr, tx_tr)\n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    \n",
    "    rmse_tr = np.sqrt(2 * mse_tr)\n",
    "    rmse_te = np.sqrt(2 * compute_loss(y_te, tx_te, w))\n",
    "    print(\n",
    "        \"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "            p=ratio, d=degree, tr=rmse_tr, te=rmse_te\n",
    "        )\n",
    "    )\n",
    "    return x_tr, x_te, y_tr, y_te, w\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca289211-ee99-4e49-beab-33b71d47b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"visualization cross validation least squares \"\"\"\n",
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.7, 0.5, 0.1]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 4\n",
    "num_col = 4\n",
    "axs = plt.subplots(num_row, num_col, figsize=(20, 8))[1]\n",
    "\n",
    "for ind, split_ratio in enumerate(split_ratios):\n",
    "    for ind_d, degree in enumerate(degrees):\n",
    "        x_tr, x_te, y_tr, y_te, w = train_test_split_demo(\n",
    "            x, y, degree, split_ratio, seed\n",
    "        )\n",
    "        plot_fitted_curve(y_tr, x_tr, w, degree, axs[ind_d][ind % num_col])\n",
    "        axs[ind_d][ind].set_title(f\"Degree: {degree}, Split {split_ratio}\")\n",
    "plt.tight_layout()\n",
    "### SOLUTION\n",
    "plt.savefig(\"split_demo.png\".format(ind))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be6975-e51f-4708-bcf1-1e20a565badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tentative d'écrire une fonction de cross validation pour least squares, mais celle du cours marche déjà très bien\"\"\"\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, lambda_, method=\"ridge\", max_iters=None, gamma=None):\n",
    "    \"\"\"Return the loss of regression for a fold corresponding to k_indices.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), where N is the number of samples.\n",
    "        x: numpy array of shape (N, D), where D is the number of features.\n",
    "        k_indices: 2D array returned by build_k_indices.\n",
    "        k: scalar, the k-th fold.\n",
    "        degree: scalar, degree of the polynomial expansion.\n",
    "        lambda_: scalar, regularization parameter.\n",
    "        method: string, regression method (\"ridge\", \"least_squares\", or \"mean_squared_error_gd\").\n",
    "        max_iters: scalar, the maximum number of iterations for gradient descent (for MSE).\n",
    "        gamma: scalar, the stepsize for gradient descent (for MSE).\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors (RMSE).\n",
    "    \"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    te_indices = k_indices[k]\n",
    "    tr_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indices = tr_indices.reshape(-1)\n",
    "    y_te = y[te_indices]\n",
    "    y_tr = y[tr_indices]\n",
    "    x_te = x[te_indices]\n",
    "    x_tr = x[tr_indices]\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    if method == \"ridge\":\n",
    "        # Ridge regression\n",
    "        w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    elif method == \"least_squares\":\n",
    "        # Least squares regression\n",
    "        w = least_squares(y_tr, tx_tr)[0]\n",
    "    elif method == \"mean_squared_error_gd\":\n",
    "        # Mean squared error gradient descent\n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        w, _ = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression method. Use 'ridge', 'least_squares', or 'mean_squared_error_gd'.\")\n",
    "\n",
    "    # Calculate the RMSE for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, w))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "    return loss_tr, loss_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93d3fa-1df7-4501-baf7-c3534a66a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cross validation avec mean_squared_error_gd, least squares(qui n'est pas debugé, maisdont la fonction du cours marche) et ridge\"\"\"\n",
    "\n",
    "\n",
    "#cette fonction cross validation sera à terme une seule fonction pour chaque méthode d'optimization mais pour l'instant on a commencé à l'écrire pour mean square, ridge et mean gradient, mais il n'est pas débuggé attention!!\n",
    "def cross_validation(y, x, k_indices, k, degree, lambda_, method=\"ridge\", max_iters=None, gamma=None):\n",
    "    \"\"\"Return the loss of regression for a fold corresponding to k_indices.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), where N is the number of samples.\n",
    "        x: numpy array of shape (N, D), where D is the number of features.\n",
    "        k_indices: 2D array returned by build_k_indices.\n",
    "        k: scalar, the k-th fold.\n",
    "        degree: scalar, degree of the polynomial expansion.\n",
    "        lambda_: scalar, regularization parameter.\n",
    "        method: string, regression method (\"ridge\", \"least_squares\", or \"mean_squared_error_gd\").\n",
    "        max_iters: scalar, the maximum number of iterations for gradient descent (for MSE).\n",
    "        gamma: scalar, the stepsize for gradient descent (for MSE).\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors (RMSE).\n",
    "    \"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    te_indices = k_indices[k]\n",
    "    tr_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indices = tr_indices.reshape(-1)\n",
    "    y_te = y[te_indices]\n",
    "    y_tr = y[tr_indices]\n",
    "    x_te = x[te_indices]\n",
    "    x_tr = x[tr_indices]\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    if method == \"ridge\":\n",
    "        # Ridge regression\n",
    "        w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    elif method == \"least_squares\":\n",
    "        # Least squares regression\n",
    "        w = least_squares(y_tr, tx_tr)[0]\n",
    "    elif method == \"mean_squared_error_gd\":\n",
    "        # Mean squared error gradient descent\n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        w, _ = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression method. Use 'ridge', 'least_squares', or 'mean_squared_error_gd'.\")\n",
    "\n",
    "    # Calculate the RMSE for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, w))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "    return loss_tr, loss_te\n",
    "\n",
    "import numpy as np\n",
    "from helpers import split_data, build_k_indices, cross_validation\n",
    "from implementations import mean_squared_error_gd\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "max_iters_values = [10, 50, 100, 500, 1000]\n",
    "gamma_values = [0.001, 0.01, 0.1, 0.2, 0.5]\n",
    "degree_values = [1, 3, 5, 7]\n",
    "\n",
    "# Initialize results dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Perform cross-validation for each combination of hyperparameters\n",
    "for max_iters in max_iters_values:\n",
    "    for gamma in gamma_values:\n",
    "        for degree in degree_values:\n",
    "            # Initialize a list to store the results for each fold\n",
    "            rmse_tr_list = []\n",
    "            rmse_te_list = []\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                # Split the data into training and testing sets\n",
    "                x_tr, x_te, y_tr, y_te = split_data(x, y, k_indices, k)\n",
    "\n",
    "                # Apply mean squared error gradient descent with the current hyperparameters\n",
    "                initial_w = np.zeros(x.shape[1])\n",
    "                w, loss = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "                # Calculate RMSE for training and testing data\n",
    "                rmse_tr, rmse_te = cross_validation(y_tr, x_tr, k_indices, k, degree, 0.0, method=\"mean_squared_error_gd\", max_iters=max_iters, gamma=gamma)\n",
    "\n",
    "                rmse_tr_list.append(rmse_tr)\n",
    "                rmse_te_list.append(rmse_te)\n",
    "\n",
    "            # Store the mean RMSE values for this combination of hyperparameters\n",
    "            results[(max_iters, gamma, degree)] = {\n",
    "                \"rmse_tr\": np.mean(rmse_tr_list),\n",
    "                \"rmse_te\": np.mean(rmse_te_list),\n",
    "            }\n",
    "\n",
    "# Find the hyperparameters with the lowest test RMSE\n",
    "best_hyperparameters = min(results, key=lambda x: results[x][\"rmse_te\"])\n",
    "best_max_iters, best_gamma, best_degree = best_hyperparameters\n",
    "best_rmse_te = results[best_hyperparameters][\"rmse_te\"]\n",
    "\n",
    "print(f\"Best hyperparameters: max_iters = {best_max_iters}, gamma = {best_gamma}, degree = {best_degree}\")\n",
    "print(f\"Corresponding test RMSE: {best_rmse_te}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fbc65-0441-486a-85c3-7939c9433389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cross validation avec mean_squared_error_sgd\"\"\"\n",
    "\n",
    "#encore une fois j'ai adapté la fonction avec le sgd et chat gpt a un peu aidé j'avoue, pas encore débuggé attention\n",
    "def cross_validation(y, x, k_indices, k, degree, lambda_, method=\"ridge\", max_iters=None, gamma=None, batch_size=None):\n",
    "    \"\"\"Return the loss of regression for a fold corresponding to k_indices.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), where N is the number of samples.\n",
    "        x: numpy array of shape (N, D), where D is the number of features.\n",
    "        k_indices: 2D array returned by build_k_indices.\n",
    "        k: scalar, the k-th fold.\n",
    "        degree: scalar, degree of the polynomial expansion.\n",
    "        lambda_: scalar, regularization parameter.\n",
    "        method: string, regression method (\"ridge\", \"least_squares\", \"mean_squared_error_gd\", or \"mean_squared_error_sgd\").\n",
    "        max_iters: scalar, the maximum number of iterations for gradient descent (for MSE).\n",
    "        gamma: scalar, the stepsize for gradient descent (for MSE).\n",
    "        batch_size: scalar, the batch size for stochastic gradient descent (for MSE).\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors (RMSE).\n",
    "    \"\"\"\n",
    "    # Get k'th subgroup in test, others in train\n",
    "    te_indices = k_indices[k]\n",
    "    tr_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indices = tr_indices.reshape(-1)\n",
    "    y_te = y[te_indices]\n",
    "    y_tr = y[tr_indices]\n",
    "    x_te = x[te_indices]\n",
    "    x_tr = x[tr_indices]\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    if method == \"ridge\":\n",
    "        # Ridge regression\n",
    "        w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    elif method == \"least_squares\":\n",
    "        # Least squares regression\n",
    "        w = least_squares(y_tr, tx_tr)[0]\n",
    "    elif method == \"mean_squared_error_gd\":\n",
    "        # Mean squared error gradient descent\n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        w, _ = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "    elif method == \"mean_squared_error_sgd\":\n",
    "        # Mean squared error stochastic gradient descent\n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        w, _ = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma, batch_size)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression method. Use 'ridge', 'least_squares', 'mean_squared_error_gd', or 'mean_squared_error_sgd'.\")\n",
    "\n",
    "    # Calculate the RMSE for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, w))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\"\"\"Calcul des hyperparameters\"\"\" #attention pour le tuning il faudra absolument utiliser des subsamples pq sinon la boucle for imbriquées 5 fois va détruire la planète pendant les 10 prochaines années\n",
    "import numpy as np\n",
    "from helpers import split_data, build_k_indices, cross_validation\n",
    "from implementations import mean_squared_error_gd, mean_squared_error_sgd  # Make sure you import the SGD function\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "max_iters_values = [10, 50, 100, 500, 1000]\n",
    "gamma_values = [0.001, 0.01, 0.1, 0.2, 0.5]\n",
    "degree_values = [1, 3, 5, 7]\n",
    "batch_size_values = [1, 16, 32, 64]  # Batch sizes for SGD\n",
    "\n",
    "# Initialize results dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Perform cross-validation for each combination of hyperparameters\n",
    "for max_iters in max_iters_values:\n",
    "    for gamma in gamma_values:\n",
    "        for degree in degree_values:\n",
    "            for batch_size in batch_size_values:\n",
    "                # Initialize a list to store the results for each fold\n",
    "                rmse_tr_list = []\n",
    "                rmse_te_list = []\n",
    "\n",
    "                for k in range(k_fold):\n",
    "                    # Split the data into training and testing sets\n",
    "                    x_tr, x_te, y_tr, y_te = split_data(x, y, k_indices, k)\n",
    "\n",
    "                    # Apply the selected optimization method with the current hyperparameters\n",
    "                    initial_w = np.zeros(x_tr.shape[1])\n",
    "                    if method == \"mean_squared_error_gd\":\n",
    "                        w, _ = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "                    elif method == \"mean_squared_error_sgd\":\n",
    "                        w, _ = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iters, gamma, batch_size)\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid optimization method. Use 'mean_squared_error_gd' or 'mean_squared_error_sgd'.\")\n",
    "\n",
    "                    # Calculate RMSE for training and testing data\n",
    "                    rmse_tr, rmse_te = cross_validation(y_tr, x_tr, k_indices, k, degree, lambda_, method, max_iters, gamma, batch_size)\n",
    "\n",
    "                    rmse_tr_list.append(rmse_tr)\n",
    "                    rmse_te_list.append(rmse_te)\n",
    "\n",
    "                # Store the mean RMSE values for this combination of hyperparameters\n",
    "                results[(max_iters, gamma, degree, batch_size)] = {\n",
    "                    \"rmse_tr\": np.mean(rmse_tr_list),\n",
    "                    \"rmse_te\": np.mean(rmse_te_list),\n",
    "                }\n",
    "\n",
    "# Find the hyperparameters with the lowest test RMSE\n",
    "best_hyperparameters = min(results, key=lambda x: results[x][\"rmse_te\"])\n",
    "best_max_iters, best_gamma, best_degree, best_batch_size = best_hyperparameters\n",
    "best_rmse_te = results[best_hyperparameters][\"rmse_te\"]\n",
    "\n",
    "print(f\"Best hyperparameters: max_iters = {best_max_iters}, gamma = {best_gamma}, degree = {best_degree}, batch_size = {best_batch_size}\")\n",
    "print(f\"Corresponding test RMSE: {best_rmse_te}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31252a9d-1d83-4b44-973f-ecda7e4af36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" cross validation avec logistic_regression \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def cross_validation_logistic(y, tx, k_fold, initial_weights, max_iters, gammas):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to tune hyperparameters for logistic regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        k_fold: Number of cross-validation folds\n",
    "        initial_weights: List of initial weight vectors to try\n",
    "        max_iters: List of max_iters values to try\n",
    "        gammas: List of gamma values to try\n",
    "\n",
    "    Returns:\n",
    "        best_hyperparameters: Dictionary with the best hyperparameters\n",
    "        best_loss: The lowest loss obtained during cross-validation\n",
    "    \"\"\"\n",
    "    num_samples = len(y)\n",
    "    num_features = tx.shape[1]\n",
    "    fold_size = num_samples // k_fold\n",
    "    best_loss = float(\"inf\")\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    for initial_w in initial_weights:\n",
    "        for max_iter in max_iters:\n",
    "            for gamma in gammas:\n",
    "                total_loss = 0.0\n",
    "\n",
    "                for fold in range(k_fold):\n",
    "                    # Split the data into training and validation sets\n",
    "                    val_start = fold * fold_size\n",
    "                    val_end = (fold + 1) * fold_size\n",
    "                    val_indices = np.arange(val_start, val_end)\n",
    "                    train_indices = np.delete(np.arange(num_samples), val_indices)\n",
    "\n",
    "                    y_train, tx_train = y[train_indices], tx[train_indices]\n",
    "                    y_val, tx_val = y[val_indices], tx[val_indices]\n",
    "\n",
    "                    # Train the model on the training data\n",
    "                    w, _ = logistic_regression(y_train, tx_train, initial_w, max_iter, gamma)\n",
    "\n",
    "                    # Calculate loss on the validation set\n",
    "                    val_loss = compute_loss(y_val, tx_val, w, \"log\")\n",
    "                    total_loss += val_loss\n",
    "\n",
    "                average_loss = total_loss / k_fold\n",
    "\n",
    "                if average_loss < best_loss:\n",
    "                    best_loss = average_loss\n",
    "                    best_hyperparameters = {\n",
    "                        'initial_w': initial_w,\n",
    "                        'max_iters': max_iter,\n",
    "                        'gamma': gamma,\n",
    "                    }\n",
    "\n",
    "    return best_hyperparameters, best_loss\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "initial_weights = [initial_w1, initial_w2]  # Define your initial weights\n",
    "max_iters = [100, 500, 1000]\n",
    "gammas = [0.01, 0.1, 1.0]\n",
    "k_fold = 5  # You can adjust the number of cross-validation folds\n",
    "\n",
    "# Perform cross-validation\n",
    "best_hyperparameters, best_loss = cross_validation_logistic(y, tx, k_fold, initial_weights, max_iters, gammas)\n",
    "\n",
    "# The best hyperparameters will be in best_hyperparameters\n",
    "# The lowest cross-validation loss will be in best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973d3a1-c2d4-42e0-8275-eea41015b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cross validation avec reg_logistic_regression\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def cross_validation(y, tx, k_fold, lambdas, initial_weights, max_iters, gammas):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to tune hyperparameters for regularized logistic regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        k_fold: Number of cross-validation folds\n",
    "        lambdas: List of lambda values to try\n",
    "        initial_weights: List of initial weight vectors to try\n",
    "        max_iters: List of max_iters values to try\n",
    "        gammas: List of gamma values to try\n",
    "\n",
    "    Returns:\n",
    "        best_hyperparameters: Dictionary with the best hyperparameters\n",
    "        best_loss: The lowest loss obtained during cross-validation\n",
    "    \"\"\"\n",
    "    num_samples = len(y)\n",
    "    num_features = tx.shape[1]\n",
    "    fold_size = num_samples // k_fold\n",
    "    best_loss = float(\"inf\")\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    for lambda_ in lambdas:\n",
    "        for initial_w in initial_weights:\n",
    "            for max_iter in max_iters:\n",
    "                for gamma in gammas:\n",
    "                    total_loss = 0.0\n",
    "\n",
    "                    for fold in range(k_fold):\n",
    "                        # Split the data into training and validation sets\n",
    "                        val_start = fold * fold_size\n",
    "                        val_end = (fold + 1) * fold_size\n",
    "                        val_indices = np.arange(val_start, val_end)\n",
    "                        train_indices = np.delete(np.arange(num_samples), val_indices)\n",
    "\n",
    "                        y_train, tx_train = y[train_indices], tx[train_indices]\n",
    "                        y_val, tx_val = y[val_indices], tx[val_indices]\n",
    "\n",
    "                        # Train the model on the training data\n",
    "                        w, _ = reg_logistic_regression(y_train, tx_train, lambda_, initial_w, max_iter, gamma)\n",
    "\n",
    "                        # Calculate loss on the validation set\n",
    "                        val_loss = compute_loss(y_val, tx_val, w, \"log\")\n",
    "                        total_loss += val_loss\n",
    "\n",
    "                    average_loss = total_loss / k_fold\n",
    "\n",
    "                    if average_loss < best_loss:\n",
    "                        best_loss = average_loss\n",
    "                        best_hyperparameters = {\n",
    "                            'lambda': lambda_,\n",
    "                            'initial_w': initial_w,\n",
    "                            'max_iters': max_iter,\n",
    "                            'gamma': gamma,\n",
    "                        }\n",
    "\n",
    "    return best_hyperparameters, best_loss\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "lambdas = [0.01, 0.1, 1.0]\n",
    "initial_weights = [initial_w1, initial_w2]  # Define your initial weights\n",
    "max_iters = [100, 500, 1000]\n",
    "gammas = [0.01, 0.1, 1.0]\n",
    "k_fold = 5  # You can adjust the number of cross-validation folds\n",
    "\n",
    "# Perform cross-validation\n",
    "best_hyperparameters, best_loss = cross_validation(y, tx, k_fold, lambdas, initial_weights, max_iters, gammas)\n",
    "\n",
    "# The best hyperparameters will be in best_hyperparameters\n",
    "# The lowest cross-validation loss will be in best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c60d58-b7fd-447c-ae3a-0490394d9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calculer le f1 score\"\"\"\n",
    "\n",
    "\n",
    "def compute_f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_pred == 1) & (y_pred == y_true))\n",
    "    fp = np.sum((y_pred == 1) & (y_pred != y_true))\n",
    "    fn = np.sum((y_pred == 0) & (y_pred != y_true))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a646257-89ee-43a4-9102-b335d2dedf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calculer l'accuracy\"\"\"\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8e3ae-91c3-4c16-adee-c91c06b34cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fonction sigmoid pour optimiser la probabilité que la rep soit vraie\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235a217-050b-4673-877b-811bcd35b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"prédire le y\"\"\"\n",
    "\n",
    "def predict(self, x, eval_mode=False):\n",
    "        if eval_mode:\n",
    "            n_feat = x.shape[1]\n",
    "            x = build_poly(x, self.degree)\n",
    "            x[:,n_feat:] = (x[:,n_feat:] - self.mean)/self.std\n",
    "        \n",
    "        y_pred = np.empty_like(x @ self.weights)\n",
    "        y_pred[sigmoid(x @ self.weights) > 0.5] = 1\n",
    "        y_pred[sigmoid(x @ self.weights) <= 0.5] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d0d3c-3fe8-4e82-94f4-fbcba727916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selon les différentes méthodes de régression on à différents critères de prédiction : cf série 5 on a dans la fonction de visualisation différents critères pour logistic reg et pour least squares\n",
    "\"\"\"idées pour faire la classification\"\"\"\n",
    " # The threshold should be different for least squares and logistic regression when label is {0,1}.\n",
    "    # least square: decision boundary t >< 0.5\n",
    "    # logistic regression:  decision boundary sigmoid(t) >< 0.5  <==> t >< 0\n",
    "    if is_LR:\n",
    "        prediction = x_temp.dot(w) > 0.0\n",
    "    else:\n",
    "        prediction = x_temp.dot(w) > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e911c-5ec1-4482-b007-8d818a2ffb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"créer la submission\"\"\"\n",
    "\n",
    "create_csv_submission(ids_pred, y_pred, DATA_PATH + \"submission.csv\")\n",
    "print(\"Submission saved at 'data/submission.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
