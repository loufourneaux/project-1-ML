{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd5b8a-bf87-484f-bef2-35c0d6609448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac4c19-6a82-40e6-8063-14bb1ba4d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_with_degree(y, tx, k_fold, hyperparameters, method=\"ridge\"):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to tune hyperparameters for different regression methods, including degree.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N,)\n",
    "        tx: numpy array of shape=(N, D)\n",
    "        k_fold: Number of cross-validation folds\n",
    "        hyperparameters: Dictionary containing hyperparameters specific to the selected method,\n",
    "            including \"degree\" for polynomial regression methods.\n",
    "        method: string, the name of the regression method (\"ridge\", \"least_squares\", \"mean_squared_error_gd\", \"mean_squared_error_sgd\", \"logistic\", or \"reg_logistic\").\n",
    "\n",
    "    Returns:\n",
    "        best_hyperparameters: Dictionary with the best hyperparameters\n",
    "        best_loss: The lowest loss obtained during cross-validation\n",
    "    \"\"\"\n",
    "    num_samples = len(y)\n",
    "    fold_size = num_samples // k_fold\n",
    "    best_loss = float(\"inf\")\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    for hyperparameter_set in hyperparameters:\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for fold in range(k_fold):\n",
    "            val_start = fold * fold_size\n",
    "            val_end = (fold + 1) * fold_size\n",
    "            val_indices = np.arange(val_start, val_end)\n",
    "            train_indices = np.delete(np.arange(num_samples), val_indices)\n",
    "\n",
    "            y_train, tx_train = y[train_indices], tx[train_indices]\n",
    "            y_val, tx_val = y[val_indices], tx[val_indices]\n",
    "\n",
    "            if \"degree\" in hyperparameter_set:\n",
    "                degree = hyperparameter_set[\"degree\"]\n",
    "                tx_train = build_poly(tx_train, degree)\n",
    "                tx_val = build_poly(tx_val, degree)\n",
    "\n",
    "            if method == \"ridge\":\n",
    "                lambda_ = hyperparameter_set[\"lambda\"]\n",
    "                w = ridge_regression(y_train, tx_train, lambda_)\n",
    "            elif method == \"least_squares\":\n",
    "                w, _ = least_squares(y_train, tx_train)\n",
    "            elif method == \"mean_squared_error_gd\":\n",
    "                initial_w = hyperparameter_set[\"initial_w\"]\n",
    "                max_iters = hyperparameter_set[\"max_iters\"]\n",
    "                gamma = hyperparameter_set[\"gamma\"]\n",
    "                w, _ = mean_squared_error_gd(y_train, tx_train, initial_w, max_iters, gamma)\n",
    "            elif method == \"mean_squared_error_sgd\":\n",
    "                initial_w = hyperparameter_set[\"initial_w\"]\n",
    "                max_iters = hyperparameter_set[\"max_iters\"]\n",
    "                gamma = hyperparameter_set[\"gamma\"]\n",
    "                batch_size = hyperparameter_set[\"batch_size\"]\n",
    "                w, _ = mean_squared_error_sgd(y_train, tx_train, initial_w, max_iters, gamma, batch_size)\n",
    "            elif method == \"logistic\":\n",
    "                initial_w = hyperparameter_set[\"initial_w\"]\n",
    "                max_iters = hyperparameter_set[\"max_iters\"]\n",
    "                gamma = hyperparameter_set[\"gamma\"]\n",
    "                w, _ = logistic_regression(y_train, tx_train, initial_w, max_iters, gamma)\n",
    "            elif method == \"reg_logistic\":\n",
    "                lambda_ = hyperparameter_set[\"lambda\"]\n",
    "                initial_w = hyperparameter_set[\"initial_w\"]\n",
    "                max_iters = hyperparameter_set[\"max_iters\"]\n",
    "                gamma = hyperparameter_set[\"gamma\"]\n",
    "                w, _ = reg_logistic_regression(y_train, tx_train, lambda_, initial_w, max_iters, gamma)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid regression method.\")\n",
    "\n",
    "            if method in [\"ridge\", \"logistic\", \"reg_logistic\"]:\n",
    "                val_loss = compute_loss(y_val, tx_val, w, \"log\")\n",
    "            else:\n",
    "                val_loss = np.sqrt(2 * compute_mse(y_val, tx_val, w))\n",
    "\n",
    "            total_loss += val_loss\n",
    "\n",
    "        average_loss = total_loss / k_fold\n",
    "\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            best_hyperparameters = hyperparameter_set\n",
    "\n",
    "    return best_hyperparameters, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4796b5-0b9e-47b5-929d-1dc12f41794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of hyperparameters for different regression methods\n",
    "hyperparameters = []\n",
    "\n",
    "# Ridge Regression Hyperparameters\n",
    "ridge_hyperparams = [\n",
    "    {\"method\": \"ridge\", \"lambda\": 0.1, \"degree\": 1},\n",
    "    {\"method\": \"ridge\", \"lambda\": 0.01, \"degree\": 2},\n",
    "    # Add more hyperparameters for ridge regression as needed\n",
    "]\n",
    "hyperparameters.extend(ridge_hyperparams)\n",
    "\n",
    "# Least Squares Hyperparameters\n",
    "least_squares_hyperparams = [\n",
    "    {\"method\": \"least_squares\"},\n",
    "    # Add more hyperparameters for least squares as needed\n",
    "]\n",
    "hyperparameters.extend(least_squares_hyperparams)\n",
    "\n",
    "# Mean Squared Error (Gradient Descent) Hyperparameters\n",
    "mse_gd_hyperparams = [\n",
    "    {\"method\": \"mean_squared_error_gd\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for MSE with GD as needed\n",
    "]\n",
    "hyperparameters.extend(mse_gd_hyperparams)\n",
    "\n",
    "# Mean Squared Error (Stochastic Gradient Descent) Hyperparameters\n",
    "mse_sgd_hyperparams = [\n",
    "    {\"method\": \"mean_squared_error_sgd\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"batch_size\": batch_size_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for MSE with SGD as needed\n",
    "]\n",
    "hyperparameters.extend(mse_sgd_hyperparams)\n",
    "\n",
    "# Logistic Regression Hyperparameters\n",
    "logistic_hyperparams = [\n",
    "    {\"method\": \"logistic\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for logistic regression as needed\n",
    "]\n",
    "hyperparameters.extend(logistic_hyperparams)\n",
    "\n",
    "# Regularized Logistic Regression Hyperparameters\n",
    "reg_logistic_hyperparams = [\n",
    "    {\"method\": \"reg_logistic\", \"lambda\": 0.1, \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for regularized logistic regression as needed\n",
    "]\n",
    "hyperparameters.extend(reg_logistic_hyperparams)\n",
    "\n",
    "# You can continue to add more hyperparameters as needed for each method.\n",
    "\n",
    "# Now, the `hyperparameters` list contains sets of hyperparameters for all six methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bd02d-83a0-4d39-9697-da1cef02afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of hyperparameters for each regression method\n",
    "ridge_hyperparams = [\n",
    "    {\"method\": \"ridge\", \"lambda\": 0.1, \"degree\": 1},\n",
    "    {\"method\": \"ridge\", \"lambda\": 0.01, \"degree\": 2},\n",
    "    # Add more hyperparameters for ridge regression as needed\n",
    "]\n",
    "\n",
    "least_squares_hyperparams = [\n",
    "    {\"method\": \"least_squares\"},\n",
    "    # Add more hyperparameters for least squares as needed\n",
    "]\n",
    "\n",
    "mse_gd_hyperparams = [\n",
    "    {\"method\": \"mean_squared_error_gd\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for MSE with GD as needed\n",
    "]\n",
    "\n",
    "mse_sgd_hyperparams = [\n",
    "    {\"method\": \"mean_squared_error_sgd\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"batch_size\": batch_size_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for MSE with SGD as needed\n",
    "]\n",
    "\n",
    "logistic_hyperparams = [\n",
    "    {\"method\": \"logistic\", \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for logistic regression as needed\n",
    "]\n",
    "\n",
    "reg_logistic_hyperparams = [\n",
    "    {\"method\": \"reg_logistic\", \"lambda\": 0.1, \"initial_w\": initial_w_value, \"max_iters\": max_iters_value, \"gamma\": gamma_value, \"degree\": 1},\n",
    "    # Add more hyperparameters for regularized logistic regression as needed\n",
    "]\n",
    "\n",
    "# Call cross_validation_with_degree for each regression method\n",
    "best_hyperparams_ridge, best_loss_ridge = cross_validation_with_degree(y, tx, k_fold, ridge_hyperparams, method=\"ridge\")\n",
    "best_hyperparams_least_squares, best_loss_least_squares = cross_validation_with_degree(y, tx, k_fold, least_squares_hyperparams, method=\"least_squares\")\n",
    "best_hyperparams_mse_gd, best_loss_mse_gd = cross_validation_with_degree(y, tx, k_fold, mse_gd_hyperparams, method=\"mean_squared_error_gd\")\n",
    "best_hyperparams_mse_sgd, best_loss_mse_sgd = cross_validation_with_degree(y, tx, k_fold, mse_sgd_hyperparams, method=\"mean_squared_error_sgd\")\n",
    "best_hyperparams_logistic, best_loss_logistic = cross_validation_with_degree(y, tx, k_fold, logistic_hyperparams, method=\"logistic\")\n",
    "best_hyperparams_reg_logistic, best_loss_reg_logistic = cross_validation_with_degree(y, tx, k_fold, reg_logistic_hyperparams, method=\"reg_logistic\")\n",
    "\n",
    "# You can access the best hyperparameters and losses for each method as needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
